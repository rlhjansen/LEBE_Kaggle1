"""

#
# Todo:
#

Kopieer "train_val_plot_thresh.py", "pipeline_everything.py", "splitfile_traindata.py" en preprocessing_thresh.py
Vervang de main functie (onderaan) in "train_val_plot_thresh.py" door de eerste main functie waar je naam bij staat.
Maak een tekstbestand zet daarin de main die bij je naam staat. (zodat we naderhand weten wat voor data verzameld is)

Jochem: main(layers, act='relu', solv='adam', mini_batch_size=min(50, n_samples))
Maurits: main(layers, act='relu', solv='sgd')
Shelby: main(layers, act='relu', solv='sgd', mini_batch_size=min(50, n_samples))

Run nu "pipeline_everything.py"

Je computer zal nu een flinke tijd bezig zijn ~ 6+ uur.
Hij is bezig met het preprocessen van alle data, dit hoef je maar 1 keer te doen.

Als het niet de eerste keer is, en je dus je tweede main functie runt,
kopieer dan de volgende files naar een volgende nieuwe map:

"train_part_100.tsv"
"train_test_100.tsv"
"val_test_100.tsv"
"input_specs_100.tsv"
"train_val_plot_thresh.py"

Vervang in de nieuwe map in het bestand "train_val_plot_thresh.py"
je main functie door de volgende die bij je naam staat (hier verder onder).

Run nu "train_val_plot_thresh.py" in deze nieuwe map.
maak ook weer een tekstfile in deze map aan waarin de mainfunctie staat die je runt.

deze apparte mappen zijn er voor om verzamelde data los van elkaar te houden zodat we weten wat er onderzocht is.



volgende main functies:
Reitze: main(layers, act='tanh', solv='adam', mini_batch_size='auto')
Jochem: main(layers, act='tanh', solv='adam', mini_batch_size=min(50, n_samples))
Maurits: main(layers, act='tanh', solv='sgd')
Shelby: main(layers, act='tanh', solv='sgd', mini_batch_size=min(50, n_samples))


Reitze: main(layers, act='logistic', solv='adam', mini_batch_size='auto')
Jochem: main(layers, act='logistic', solv='adam', mini_batch_size=min(50, n_samples))
Maurits: main(layers, act='logistic', solv='sgd')
Shelby: main(layers, act='logistic', solv='sgd', mini_batch_size=min(50, n_samples))


Reitze: main(layers, act='identity', solv='adam', mini_batch_size='auto')
Jochem: main(layers, act='identity', solv='adam', mini_batch_size=min(50, n_samples))
Maurits: main(layers, act='identity', solv='sgd')
Shelby: main(layers, act='identity', solv='sgd', mini_batch_size=min(50, n_samples))



###################################################################
#                                                                 #
#       het volgende deel kan je overslaan, maar kan              #
#       je lezen als je wil weten wat je aan het doen bent        #
#                                                                 #
###################################################################

options per keyword:

act: {activation - function that determines hidden layer activation}
default = 'relu', gedaan

'identity', no-op activation, useful to implement linear bottleneck, returns f(x) = x
'logistic', the logistic sigmoid function, returns f(x) = 1 / (1 + exp(-x)).
'tanh', the hyperbolic tan function, returns f(x) = tanh(x).
'relu', the rectified linear unit function, returns f(x) = max(0, x)


solv: {solver - function that determines weight correction}
default = 'adam', gedaan

'lbfgs', is an optimizer in the family of quasi-Newton methods. (kleine dataset - niet van toepassing voor ons)
'sgd', refers to stochastic gradient descent.
'adam', refers to a stochastic gradient-based optimizer proposed by Kingma, Diederik, and Jimmy Ba

mini_batch_size: {bepaald de grootte van minibatches, default = 200, gebruikt}
voorbeeld keyword: mini_batch_size=min(200, n_samples)


de volgende instelling is al toegepast:
main(layers, act='relu', solv='adam', mini_batch_size='auto')


"""

